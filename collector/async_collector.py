import sys
import os

# Sentry SDK Integration
try:
    import sentry_sdk
    sentry_sdk.init(
        dsn=os.getenv("SENTRY_DSN"),
        traces_sample_rate=1.0,
        profiles_sample_rate=1.0,
    )
    print("Sentry initialized successfully.")
except ImportError:
    print("sentry-sdk not found. Error tracking disabled.")
except Exception as e:
    print(f"Sentry init failed: {e}")

import asyncio
import aiohttp
import email.utils
import datetime
import re

from dotenv import load_dotenv
from supabase import create_client

# [1] ÌÑ∞ÎØ∏ÎÑê ÌïúÍ∏Ä Íπ®Ïßê Î∞©ÏßÄ for Windows
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# [2] ÌôòÍ≤Ω Î≥ÄÏàò Î∞è Í≤ΩÎ°ú ÏÑ§Ï†ï
env_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(env_path)

# Keys
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# [3] Clients Initialization
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# Search Keywords (Crawl Keys)
KEYWORDS = os.getenv("KEYWORDS", "ÌïÑÎü¨,ÌÜ°Ïã†,Î≥¥ÌÜ°Ïä§,Ï•¨Î≤†Î£©,Î¶¨Ï•¨ÎûÄ,ÏóëÏÜåÏ¢Ä").split(",")

def clean_text_expert(val):
    if val is None: return "-"
    s = str(val).strip()
    # Remove HTML tags and control chars
    s = re.sub(r'<[^>]*>', '', s)
    s = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', s)
    s = s.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    return s

# [4] Naver News Fetching
async def fetch_naver_news_expert(session, keyword, start_date):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    
    all_items = []
    # [V4.6] Fetch 200 items (2 pages) to catch news without keyword in title
    for start_idx in [1, 101]:
        params = {"query": keyword, "display": 100, "sort": "date", "start": start_idx}
        try:
            async with session.get(url, headers=headers, params=params) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    items = data.get('items', [])
                    if not items: break
                    
                    for item in items:
                        pub_date = email.utils.parsedate_to_datetime(item['pubDate'])
                        if pub_date.replace(tzinfo=None) > start_date:
                            all_items.append(item)
        except Exception as e:
            print(f"  ‚ö†Ô∏è Naver API Error for [{keyword}] at start={start_idx}: {e}")
            break
    return all_items

# [5] Process Single Item (Save Raw)
async def process_news_item_expert(item, search_keyword, existing_links):
    link = item['link']
    if link in existing_links: return 0

    title = clean_text_expert(item['title'])
    desc = clean_text_expert(item['description'])
    pub_iso = email.utils.parsedate_to_datetime(item['pubDate']).isoformat()

    try:
        supabase_data = {
            "title": title,
            "description": desc,
            "link": link,
            "pub_date": pub_iso,
            "search_keyword": search_keyword,
            "status": "pending"
        }
        supabase.table("raw_news").insert(supabase_data).execute()
        print(f"üì¶ [RAW] Collected: {title[:40]}...")
        existing_links.add(link)
        return 1
    except Exception as e:
        if "duplicate key" not in str(e):
            print(f"  ‚ùå Raw Sync Error: {e}")
        return 0

# [6] Main Execution
async def main():
    # ============================================
    # üöÄ ÏãúÏûë Î∞∞ÎÑà Î∞è ÌôòÍ≤Ω Ï≤¥ÌÅ¨ (Termux Ïû¨ÏãúÏûë Ïãú ÌôïÏù∏Ïö©)
    # ============================================
    print("=" * 50)
    print("üöÄ NEWS COLLECTOR ÏãúÏûë")
    print("=" * 50)
    print(f"‚è∞ ÏãúÏûë ÏãúÍ∞Å: {datetime.datetime.now()}")
    print(f"üì¶ ÏàòÏßë ÌÇ§ÏõåÎìú: {', '.join(KEYWORDS)}")
    
    # ÌôòÍ≤Ω Ï≤¥ÌÅ¨
    env_ok = True
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("‚ùå [ENV ERROR] NAVER API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏùå!")
        env_ok = False
    else:
        print(f"‚úÖ NAVER API: ÏÑ§Ï†ïÎê® (ID: {NAVER_CLIENT_ID[:8]}...)")
    
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("‚ùå [ENV ERROR] SUPABASE ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏùå!")
        env_ok = False
    else:
        print(f"‚úÖ SUPABASE: ÏÑ§Ï†ïÎê® ({SUPABASE_URL[:30]}...)")
    
    if not env_ok:
        print("üö® ÌôòÍ≤Ω Î≥ÄÏàò Ïò§Î•òÎ°ú ÏàòÏßë Î∂àÍ∞Ä. Ï¢ÖÎ£åÌï©ÎãàÎã§.")
        return
    
    print("=" * 50)
    print("üì° Îâ¥Ïä§ ÏàòÏßë Î£®ÌîÑ ÏãúÏûë...")
    print("=" * 50)
    
    single_run = os.getenv("SINGLE_RUN", "false").lower() == "true"
    
    while True:
        try:
            print(f"\n‚è∞ Cycle Start: {datetime.datetime.now()}")
            
            # ============================================
            # [V6.0] ÎßàÏßÄÎßâ Îâ¥Ïä§ Î∞úÌñâÏãúÍ∞Ñ Í∏∞Ï§Ä ÏàòÏßë (Supabase Ï°∞Ìöå)
            # Termux Ïû¨ÏãúÏûëÌï¥ÎèÑ ÎÜìÏπú Îâ¥Ïä§ ÏóÜÏù¥ ÏàòÏßë!
            # ============================================
            
            # SupabaseÏóêÏÑú ÎßàÏßÄÎßâ ÏàòÏßëÎêú Îâ¥Ïä§Ïùò pub_date Ï°∞Ìöå (KST Í∏∞Ï§Ä)
            global_start_date = datetime.datetime(2025, 12, 19)  # Í∏∞Î≥∏ fallback
            try:
                # raw_newsÏóêÏÑú Í∞ÄÏû• ÏµúÍ∑º pub_date Ï°∞Ìöå
                last_raw = supabase.table("raw_news").select("pub_date").order("pub_date", desc=True).limit(1).execute()
                if last_raw.data and last_raw.data[0].get("pub_date"):
                    last_pub_str = last_raw.data[0]["pub_date"]
                    # ISO ÌòïÏãù ÌååÏã± (timezone Ï†ïÎ≥¥ Ï†úÍ±∞)
                    global_start_date = datetime.datetime.fromisoformat(last_pub_str.replace('Z', '+00:00').replace('+09:00', '')).replace(tzinfo=None)
                    print(f"üìÖ ÎßàÏßÄÎßâ ÏàòÏßë Îâ¥Ïä§ Î∞úÌñâÏãúÍ∞Å: {global_start_date} (Ïù¥ÌõÑ Îâ¥Ïä§Îßå ÏàòÏßë)")
                else:
                    print(f"üìÖ ÏàòÏßë Í∏∞Î°ù ÏóÜÏùå, Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©: {global_start_date}")
            except Exception as e:
                print(f"‚ö†Ô∏è ÎßàÏßÄÎßâ Î∞úÌñâÏãúÍ∞Å Ï°∞Ìöå Ïã§Ìå®: {e}, Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©")
            
            # Î™®Îì† ÌÇ§ÏõåÎìúÏóê ÎèôÏùºÌïú start_date Ï†ÅÏö© (ÎßàÏßÄÎßâ Î∞úÌñâÏãúÍ∞Å Í∏∞Ï§Ä)
            keyword_items_to_process = [(kw, global_start_date) for kw in KEYWORDS]

            # Load existing links once for the cycle (Last 1500 for speed)
            existing_links = set()
            try:
                res_raw = supabase.table("raw_news").select("link").order("created_at", desc=True).limit(1000).execute().data
                for r in res_raw: existing_links.add(r['link'])
                res_art = supabase.table("articles").select("link").order("created_at", desc=True).limit(500).execute().data
                for r in res_art: existing_links.add(r['link'])
                print(f"üìö Loaded {len(existing_links)} unique links for deduplication.")
            except Exception as e:
                print(f"‚ö†Ô∏è Link Load Error: {e}")

            async with aiohttp.ClientSession() as session:
                total_added = 0
                for keyword, start_date in keyword_items_to_process:
                    print(f"üîç Searching: [{keyword}] since {start_date}")
                    items = await fetch_naver_news_expert(session, keyword, start_date)
                    
                    added_for_kw = 0
                    for item in items:
                        added_for_kw += await process_news_item_expert(item, keyword, existing_links)
                    
                    print(f"   > Added {added_for_kw} new articles.")
                    total_added += added_for_kw

                print(f"üéâ Cycle Complete. Total Added: {total_added}")

                if single_run:
                    print("üöÄ Single run completed. Exiting.")
                    break

            print("üí§ Sleeping for 30 minutes...")
            await asyncio.sleep(1800)

        except Exception as e:
            print(f"‚ùå Error in Main Loop: {e}")
            try:
                import sentry_sdk
                sentry_sdk.capture_exception(e)
            except: pass
            await asyncio.sleep(60)
        except KeyboardInterrupt:
            break

if __name__ == "__main__":
    if sys.platform == 'win32':
         asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
