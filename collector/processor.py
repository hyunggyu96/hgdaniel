import sys
import os

# Sentry SDK Integration
try:
    import sentry_sdk
    sentry_sdk.init(
        dsn=os.getenv("SENTRY_DSN"),
        traces_sample_rate=1.0,
        profiles_sample_rate=1.0,
    )
    print("Sentry initialized successfully.")
except ImportError:
    print("sentry-sdk not found. Error tracking disabled.")
except Exception as e:
    print(f"Sentry init failed: {e}")

# Disable stdout buffering for nohup
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

# [1] í™˜ê²½ ë³€ìˆ˜ ë¨¼ì € ë¡œë“œ (InferenceEngineë³´ë‹¤ ë¨¼ì €!)
from dotenv import load_dotenv
env_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(env_path)

import json
import asyncio
import aiohttp
import datetime
from typing import List, Dict

from supabase import create_client
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from openai import AsyncOpenAI
from inference_engine import InferenceEngine

# Initialize Engines (ì´ì œ .envê°€ ë¡œë“œëœ ìƒíƒœ)
inference_manager = InferenceEngine()

# Global Stats for Self-Diagnosis
STATS = {"local": 0, "cloud": 0, "fallback": 0, "total": 0, "latencies": []}


# [2] í„°ë¯¸ë„ í•œê¸€ ê¹¨ì§ ë°©ì§€
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# [2] Load Keywords from JSON (SSOT)
shared_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '_shared')
keywords_path = os.path.join(shared_dir, 'keywords.json')
EXPERT_ANALYSIS_KEYWORDS = []

try:
    with open(keywords_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        for cat in data.get('categories', []):
            EXPERT_ANALYSIS_KEYWORDS.extend(cat.get('keywords', []))
    print(f"âœ… Loaded {len(EXPERT_ANALYSIS_KEYWORDS)} keywords from keywords.json")
except Exception as e:
    print(f"âš ï¸ Failed to load keywords.json: {e}")
    EXPERT_ANALYSIS_KEYWORDS = ["íœ´ì ¤", "ë©”ë””í†¡ìŠ¤", "íŒŒë§ˆë¦¬ì„œì¹˜", "ëŒ€ì›…ì œì•½", "í•„ëŸ¬", "ë³´í†¡ìŠ¤"]

# [2.5] Category Mapping Logic (Sync with Frontend lib/constants.ts)
CATEGORIES_CONFIG = []
try:
    with open(keywords_path, 'r', encoding='utf-8') as f:
        CATEGORIES_CONFIG = json.load(f).get('categories', [])
except: pass

# [NEW] ê¸°ì—…ëª… ë‹¨ë… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì´ í‚¤ì›Œë“œë§Œ ìˆìœ¼ë©´ Corporate News)
COMPANY_ONLY_KEYWORDS = [
    "íŒŒë§ˆë¦¬ì„œì¹˜", "íœ´ì ¤", "ë©”ë””í†¡ìŠ¤", "ì œí…Œë§ˆ", "ëŒ€ì›…ì œì•½", "ë™êµ­ì œì•½", 
    "ì¢…ê·¼ë‹¹", "ì¢…ê·¼ë‹¹ë°”ì´ì˜¤", "íœ´ë©”ë”•ìŠ¤", "íœ´ì˜¨ìŠ¤", "ì¼€ì–´ì  ",
    "ê°ˆë”ë§ˆ", "ë©€ì¸ ", "ì•¨ëŸ¬ê°„", "ì‹œì§€ë°”ì´ì˜¤", "í•œìŠ¤ë°”ì´ì˜¤ë©”ë“œ",
    "ë°”ì´ì˜¤í”ŒëŸ¬ìŠ¤", "ì›í…", "í´ë˜ì‹œìŠ¤", "ì œì´ì‹œìŠ¤ë©”ë””ì¹¼", "ë¦¬íˆ¬ì˜¤"
]

def determine_category(title, description, search_keyword):
    """Determines news category based on keyword scores (Matches Frontend Logic)"""
    content = f"{title or ''} {search_keyword or ''} {description or ''}"
    best_category = "Corporate News"
    highest_score = 0
    category_scores = {}
    
    # [NEW] ê¸°ì—…ëª… ë‹¨ë… í‚¤ì›Œë“œ ì²´í¬ - ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ê¸°ì—…ëª…ì´ê³ , ì œí’ˆ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ Corporate News
    if search_keyword in COMPANY_ONLY_KEYWORDS:
        # ì œí’ˆëª… í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        product_keywords_in_content = []
        for config in CATEGORIES_CONFIG:
            if config['label'] != "Corporate News":
                for k in config['keywords']:
                    if k != search_keyword and k in content:
                        product_keywords_in_content.append(k)
        
        # ì œí’ˆëª…ì´ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ Corporate News
        if not product_keywords_in_content:
            return "Corporate News"
    
    # Identify corporate keywords
    corporate_config = next((c for c in CATEGORIES_CONFIG if c['label'] == "Corporate News"), None)
    corporate_keywords = corporate_config['keywords'] if corporate_config else []
    mentioned_companies = [k for k in corporate_keywords if k in content]
    is_multi_company = len(mentioned_companies) >= 2
    
    for config in CATEGORIES_CONFIG:
        label = config['label']
        keywords = config['keywords']
        score = 0
        is_corporate = (label == "Corporate News")
        
        for k in keywords:
            if search_keyword == k: score += 40  # [V5.2] ê²€ìƒ‰ì–´ ê°€ì¤‘ì¹˜ í•˜í–¥ (100 -> 40)
            if title and k in title: score += 80 # [V5.2] ì œëª© ê°€ì¤‘ì¹˜ ìƒí–¥ (50 -> 80)
            if description and k in description: score += 10
            
        if is_corporate and is_multi_company:
            score += 150
            
        category_scores[label] = score
        if score > highest_score:
            highest_score = score
            best_category = label
            
    # Rule: Product category takes priority if title matches
    if best_category == "Corporate News":
        best_product_cat = None
        max_product_score = 0
        for label, score in category_scores.items():
            if label != "Corporate News" and score > max_product_score:
                max_product_score = score
                best_product_cat = label
        if best_product_cat and max_product_score >= 50:
            best_category = best_product_cat
            
    return best_category

# Import local expert logic (Must be AFTER keyword loading or passed explicitly)
sys.path.append(os.path.dirname(__file__))
try:
    from local_keyword_extractor import extract_keywords, extract_main_keyword
except ImportError:
    print("âš ï¸ local_keyword_extractor not found. Local fallback will fail.")
    def extract_main_keyword(text, title=""): return "ê¸°íƒ€"
    def extract_keywords(text, top_n=5): return []

# Setup Clients
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

GOOGLE_SHEET_URL = os.getenv("GOOGLE_SHEET_URL")
SERVICE_ACCOUNT_FILE = os.path.join(os.path.dirname(__file__), 'service_account.json')

GEMINI_KEYS = [os.getenv("GEMINI_API_KEY"), os.getenv("GEMINI_API_KEY_2")]
GEMINI_KEYS = [k for k in GEMINI_KEYS if k]

groq_client = AsyncOpenAI(api_key=os.getenv("GROQ_API_KEY"), base_url="https://api.groq.com/openai/v1") if os.getenv("GROQ_API_KEY") else None

def get_google_sheet():
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope)
        client = gspread.authorize(creds)
        sheet = client.open_by_url(GOOGLE_SHEET_URL)
        return sheet.worksheet("Synced_Articles")  # V3.2: ìƒˆ ë™ê¸°í™” ì‹œíŠ¸ ì‚¬ìš©
    except Exception as e:
        print(f"âš ï¸ Google Sheet Warning: {e}")
        return None

import re

# [3] Filtering Configuration (Confirmed by User)
CAR_BRANDS = [
    "ë¥´ë…¸ì½”ë¦¬ì•„", "ë¥´ë…¸ì‚¼ì„±", "í˜„ëŒ€ì°¨", "ê¸°ì•„ì°¨", "ìŒìš©ì°¨", "KGëª¨ë¹Œë¦¬í‹°", "ì‰ë³´ë ˆ", 
    "í­ìŠ¤ë°”ê²", "ë©”ë¥´ì„¸ë°ìŠ¤", "ë²¤ì¸ ", "BMW", "ì•„ë¥´ì¹´ë‚˜", "í† ë ˆìŠ¤", "ê·¸ëœì €"
]

CAR_NOISE_KEYWORDS = [
    "ì‹œìŠ¹ê¸°", "ìë™ì°¨ ë¦¬ì½œ", "íƒ€ì´ì–´ êµì²´", "ë‚´ë¹„ê²Œì´ì…˜ ì—…ë°ì´íŠ¸", "ì¤‘ê³ ì°¨", "ì „ê¸°ì°¨", "ìˆ˜ì†Œì°¨",
    "ë„ë¡œê³µì‚¬", "ë¸”ë™ë°•ìŠ¤", "ë‹¹êµ¬(PBA)", "í”„ë¡œë†êµ¬", "í”„ë¡œë°°êµ¬"
]

# Keywords that are 100% Medical Aesthetic (Safe to skip AI check)
STRONG_MED_KEYWORDS = [k for k in EXPERT_ANALYSIS_KEYWORDS if "í•„ëŸ¬" in k or "í†¡ì‹ " in k or "ë¦¬ì¥¬ë€" in k]

# ğŸš« ë…¸ì´ì¦ˆ ì°¨ë‹¨ í‚¤ì›Œë“œ (ì œëª©/ë³¸ë¬¸ì— ìˆìœ¼ë©´ ì¦‰ì‹œ íê¸°)
BAD_KEYWORDS = [
    "ìºì‹œì›Œí¬", "ìºì‹œë‹¥", "ìš©ëˆí€´ì¦ˆ", "ëˆë²„ëŠ”í€´ì¦ˆ", "ì •ë‹µ", "í€´ì¦ˆ",  # ë¦¬ì›Œë“œ ì•±
    "ì‹ ì°¨", "ì œë„¤ì‹œìŠ¤", "SUV", "GV90", "A-í•„ëŸ¬", "B-í•„ëŸ¬", "C-í•„ëŸ¬", # ìë™ì°¨ (í•˜ì´í”ˆ í¬í•¨)
    "Aí•„ëŸ¬", "Bí•„ëŸ¬", "Cí•„ëŸ¬", "Dí•„ëŸ¬",  # ìë™ì°¨ í•„ëŸ¬ (í•˜ì´í”ˆ ì—†ìŒë„ ì¶”ê°€)
    "í•„ëŸ¬íˆ¬í•„ëŸ¬", "Pillar", "íŒ¨ìŠ¤íŠ¸ë°±", "1ì—´", "2ì—´",                # ìë™ì°¨ ê´€ë ¨ ëª…í™•í•œ ë…¸ì´ì¦ˆ
    "ë””ì§€í„¸í‚¤", "íŒŒë…¸ë¼ë§ˆë””ìŠ¤í”Œë ˆì´", "ì „ë™í™”", "í…ŒìŠ¬ë¼", "í˜„ëŒ€ì°¨", "ê¸°ì•„",
    "BMW", "MINI", "ì¿ í¼", "LGë””ìŠ¤í”Œë ˆì´", "ì‚¼ì„±ë””ìŠ¤í”Œë ˆì´",         # ë¸Œëœë“œ (ê°•ë ¥ ì°¨ë‹¨)
    "LGD", "í† ìš”íƒ€", "ì½”ë¡¤ë¼", "GR ", "ë ‰ì„œìŠ¤", "í˜¼ë‹¤",              # ìë™ì°¨ ë¸Œëœë“œ ì¶”ê°€ (2026-01-06)
    "ìºí„°í•„ëŸ¬", "ìºí”¼í„°í•„ëŸ¬", "ìºí„°í•„ë¼", "Caterpillar", "ë§ˆì´í¬ë¡ ", "ë¯¸ ì¦ì‹œ", # ì¦ì‹œ/ì¥ë¹„ ë…¸ì´ì¦ˆ
    # ì„¸ê¸ˆ/ì¡°ì„¸/ê¸ˆìœµ ë…¸ì´ì¦ˆ (2026-01-06 ì¶”ê°€)
    "OECDìµœì €ì„¸", "ê¸€ë¡œë²Œìµœì €í•œì„¸", "ê¸€ë¡œë²Œ ìµœì €í•œì„¸", "ì¡°ì„¸íšŒí”¼", "ë¯¸ ì¬ë¬´ë¶€", "ì¬ë¬´ë¶€ í•©ì˜",
    "JPëª¨ê±´", "JPëª¨ê°„", "í—¬ìŠ¤ì¼€ì–´ ì§‘ê²°", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¦ì‹œ", "ì£¼ê°€ì§€ìˆ˜",
    "ê¸°ì—… ë©´ì œ", "ì ìš© ë©´ì œ", "145ê°œêµ­", "150ê°œêµ­", # ì„¸ê¸ˆ ê´€ë ¨ ìˆ«ì íŒ¨í„´
    # CES/ì „ì‹œíšŒ ë…¸ì´ì¦ˆ (2026-01-06 ì¶”ê°€)
    "[CES", "CES 2026]", "CES2026",
]

# Robust Regex for Automotive Pillars (A/B/C-Pillar)
PILLAR_REGEX = re.compile(r"([A-C]\s*(-|â€”)?\s*í•„ëŸ¬|ìë™ì°¨|ì „ê¸°ì°¨|ëª¨ë¸ëª…|ì‹ ì°¨)", re.IGNORECASE)



# [New] ì§€ì‹ ë…¸íŠ¸ ë¡œë“œ í•¨ìˆ˜
def load_knowledge():
    try:
        k_path = os.path.join(shared_dir, 'ai_knowledge.md')
        if os.path.exists(k_path):
            with open(k_path, 'r', encoding='utf-8') as f:
                return f.read()
    except Exception as e:
        print(f"âš ï¸ Failed to load knowledge base: {e}")
    return ""

# AI Analysis Function
async def analyze_article_expert_async(title, description, search_keyword):
    """Refactored to use central InferenceEngine with In-Context Learning."""
    keyword_pool = ", ".join(EXPERT_ANALYSIS_KEYWORDS)
    knowledge_base = load_knowledge()
    
    system_prompt = (
        "You are an [Expert Strategic Analyst] for the Medical Aesthetic industry. Output MUST be strict JSON.\n"
        "Your mission: Extract high-precision business intelligence, but FIRST verify if the news is relevant.\n\n"
        
        "### ğŸ§  EXPERT KNOWLEDGE BASE (LEARNED RULES)\n"
        "Apply these rules STRICTLY to filter noise and improve accuracy:\n"
        f"{knowledge_base}\n\n"
        
        "### ğŸ›¡ï¸ STEP 1: CONTEXT VERIFICATION (CRITICAL)\n"
        "Based on the Knowledge Base above, check if the content is TRULY about Medical Aesthetics.\n"
        "1. **Homonym Trap**: If a keyword (e.g., 'Vaim') appears but context matches 'NOISE' rules (Hotel, Novel), Return 'ê¸°íƒ€'.\n"
        "2. **Irrelevant Domain**: Sports, Arts, General Politics -> Return 'ê¸°íƒ€'.\n\n"

        "### CORE ANALYTICAL TASKS (Only if Step 1 Passed):\n"
        "1. **Strategic Intent**: Identify if the news is about R&D progress, global expansion, or competition.\n"
        "2. **Keyword Governance**: You MUST only use names from the [Expert Keyword Pool] provided below.\n"
        "3. **Entity Hierarchy**: Distinguish between Parents companies (e.g., Hugel) and Brands (e.g., Letibotulinumtoxin).\n\n"
        "### STRICT EXTRACTION RULES:\n"
        "- **main_keyword**: The single most important entity from the Pool. If valid entity not found or context is noise, output 'ê¸°íƒ€'.\n"
        "- **included_keywords**: 2-4 auxiliary entities or product types from the Pool mentioned in the text.\n"
        "- **issue_nature**: Classify into one of these 8 categories:\n"
        "  - [ì œí’ˆ ì¶œì‹œ/í—ˆê°€]: New product launches, FDA/CE approvals, domestic KFDA licensing.\n"
        "  - [ì„ìƒ/ì—°êµ¬ë°ì´í„°]: Clinical trial results, academic papers, new patent registrations.\n"
        "  - [ì‹¤ì /ìˆ˜ì¶œ/ê²½ì˜]: Quarterly earnings, export volume, CEO changes, global strategy.\n"
        "  - [ë²•ì ë¶„ìŸ/ê·œì œ]: Lawsuits (ITC etc.), government sanctions, administrative actions.\n"
        "  - [íˆ¬ì/M&A]: Mergers, acquisitions, funding rounds, stock buybacks.\n"
        "  - [í•™íšŒ/ë§ˆì¼€íŒ…]: Participation in IMCAS/AMWC, sponsorship, influencer campaigns.\n"
        "  - [ê±°ì‹œê²½ì œ/ì •ì±…]: Trade policies, raw material costs, general industry trends.\n"
        "  - [ê¸°íƒ€]: Anything that doesn't fit the above or is NOISE.\n"
        "- **impact_level**: Scale 1-5 (1: Minor news, 5: Critical market-shifting event).\n\n"
        "### EXCLUSION CRITERIA (STRICT):\n"
        "- If the 'Pillar' refers to automotive parts (A/B/C pillar), construction equipment ('Caterpillar'), or general finance/semiconductor news ('Micron', 'Stock Market'), output 'issue_nature': 'ê¸°íƒ€' and 'main_keyword': 'ê¸°íƒ€'.\n"
        "- If the article is not primarily about Medical Aesthetic industry assets, treat it as noise.\n"
        "- Only output JSON. No conversational text.\n\n"
        f"### Expert Keyword Pool:\n{keyword_pool}\n"
    )
    user_prompt = f"Crawl Keyword: {search_keyword}\nHeadline: {title}\nBody: {description}"

    analysis = await inference_manager.get_analysis_hybrid(system_prompt, user_prompt)
    
    # [V5.1] AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ Fallback(ë¡œì»¬ ì¶”ì¶œ) ëŒ€ì‹  ì—ëŸ¬ë¥¼ ë°˜í™˜í•˜ì—¬ ìˆ˜ë™ ê²€í†  ìœ ë„
    return analysis

# [4] Semantic Deduplication (Point 3)
def is_semantically_duplicate(new_title, recent_articles):
    """Checks if a title is too similar to recent articles (Jaccard Similarity)."""
    def get_words(text):
        if not text: return set()
        # Remove ALL special characters including various quotes and brackets
        clean = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text)
        return set(clean.lower().split())

    new_words = get_words(new_title)
    if len(new_words) < 3: return None

    for art in recent_articles:
        ref_title = art.get('title', '')
        ref_words = get_words(ref_title)
        if not ref_words: continue
        
        intersection = new_words.intersection(ref_words)
        union = new_words.union(ref_words)
        similarity = len(intersection) / len(union) if union else 0
        
        # Lowered threshold to 0.7 to catch slightly modified titles from different agencies
        if similarity > 0.7:
            return ref_title
    return None

def is_semantic_duplicate(text1, text2, threshold=0.8):
    if not text1 or not text2: return False
    # Remove special chars and split into words
    def get_words(t): return set(re.sub(r'[^ê°€-í£a-zA-Z0-9]', ' ', t).split())
    w1 = get_words(text1)
    w2 = get_words(text2)
    if not w1 or not w2: return False
    
    intersection = w1.intersection(w2)
    union = w1.union(w2)
    similarity = len(intersection) / len(union)
    return similarity >= threshold

# [New] í‚¤ì›Œë“œë³„ ë¬¸ë§¥ ë…¸ì´ì¦ˆ ì„¤ì • (ë™ìŒì´ì˜ì–´ ë°©ì§€)
CONTEXT_NOISE_FILTER = {
    "ë°”ì„": ["í˜¸í…”", "ë¬¸í•™", "ì‘ê°€", "ì†Œì„¤", "ìš˜ í¬ì„¸", "ì¥í¸", "ì€í¬ê²½", "ì²œëª…ê´€"],
}

# [NEW] ì •í™• ë§¤ì¹­ì´ í•„ìš”í•œ í‚¤ì›Œë“œ (ë¶€ë¶„ ë§¤ì¹­ ê¸ˆì§€)
# ì˜ˆ: "ìŠ¤í‚¨" + "ë¶€ìŠ¤í„°" ë”°ë¡œ ìˆìœ¼ë©´ ë§¤ì¹­ ì•ˆë¨, "ìŠ¤í‚¨ë¶€ìŠ¤í„°" ë˜ëŠ” "ìŠ¤í‚¨ ë¶€ìŠ¤í„°"ë§Œ í—ˆìš©
EXACT_MATCH_KEYWORDS = {
    "ìŠ¤í‚¨ë¶€ìŠ¤í„°": ["ìŠ¤í‚¨ë¶€ìŠ¤í„°", "ìŠ¤í‚¨ ë¶€ìŠ¤í„°", "skinbooster", "skin booster"],
}

async def process_item(item, worksheet, recent_articles):
    raw_id = item['id']
    title = item['title']
    desc = item['description']
    link = item['link']
    pub_date = item['pub_date']
    keyword = item['search_keyword']

    # Define full_text for filtering
    full_text = f"{title} {desc}"

    # [1] Car Brands Check
    if any(brand in full_text for brand in CAR_BRANDS):
        print(f"ğŸš« Hard Filter: Car Brand detected ({title[:20]}...)")
        # Mark as processed in raw_news so we don't fetch it again, but DON'T save to articles
        supabase.table("raw_news").update({"status": "filtered"}).eq("id", raw_id).execute()
        return False

    # 2. Noise Keywords Check
    if any(noise in full_text for noise in CAR_NOISE_KEYWORDS):
        print(f"ğŸš« Hard Filter: Noise Keyword detected ({title[:20]}...)")
        supabase.table("raw_news").update({"status": "filtered"}).eq("id", raw_id).execute()
        return False
        
    # 3. Bad Keywords (Quiz, etc)
    # 3. Bad Keywords (Quiz, etc) - Check FULL TEXT
    if any(bad in full_text for bad in BAD_KEYWORDS):
        print(f"ğŸš« Hard Filter: Bad Keyword detected ({title[:20]}...)")
        supabase.table("raw_news").update({"status": "filtered"}).eq("id", raw_id).execute()
        return False

    # [NEW] 4. ì •í™• ë§¤ì¹­ í‚¤ì›Œë“œ ê²€ì¦ (ìŠ¤í‚¨ë¶€ìŠ¤í„° ë“±)
    if keyword in EXACT_MATCH_KEYWORDS:
        valid_patterns = EXACT_MATCH_KEYWORDS[keyword]
        full_text_lower = full_text.lower()
        has_exact_match = any(pattern.lower() in full_text_lower for pattern in valid_patterns)
        if not has_exact_match:
            print(f"ğŸš« Hard Filter: Exact Match Failed for '{keyword}' ({title[:20]}...)")
            supabase.table("raw_news").update({"status": "filtered"}).eq("id", raw_id).execute()
            return False

    # [1] Semantic Duplicate Check (V5.1: 80% threshold for Title OR Desc)
    for recent in recent_articles[-300:]: # Check last 300 processed items
        # Title check
        if is_semantic_duplicate(title, recent.get('title'), threshold=0.8):
            print(f"â© Skipping (Duplicate Title): {title[:30]}...")
            supabase.table("raw_news").update({"status": "duplicate"}).eq("id", raw_id).execute()
            return None
        # Description check
        if is_semantic_duplicate(desc, recent.get('description'), threshold=0.8):
            print(f"â© Skipping (Duplicate Content): {title[:30]}...")
            supabase.table("raw_news").update({"status": "duplicate"}).eq("id", raw_id).execute()
            return None

    timestamp = datetime.datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] ğŸ¤– Analyzing: {title[:40]}...")
    analysis = await analyze_article_expert_async(title, desc, keyword)
    
    # [V5.1] AI ë¶„ì„ ì‹¤íŒ¨ ì²˜ë¦¬
    if "error" in analysis:
        print(f"  âŒ AI Analysis Failed. Moving to 'ai_error' status for manual review.")
        supabase.table("raw_news").update({"status": "ai_error"}).eq("id", raw_id).execute()
        return False
    
    # Extract AI fields (safely handle dict values)
    ai_main = analysis.get("main_keyword", "ê¸°íƒ€")
    if isinstance(ai_main, dict):
        ai_main = ai_main.get("name", str(ai_main)) if "name" in ai_main else "ê¸°íƒ€"
    ai_main = str(ai_main) if ai_main else "ê¸°íƒ€"
    
    ai_included = analysis.get("included_keywords", [])
    issue_nature = analysis.get("issue_nature", "ê¸°íƒ€")
    if isinstance(issue_nature, dict):
        issue_nature = issue_nature.get("name", "ê¸°íƒ€") if "name" in issue_nature else "ê¸°íƒ€"
    issue_nature = str(issue_nature) if issue_nature else "ê¸°íƒ€"
    
    # [V4.5] AI ìš”ì•½ ëŒ€ì‹  ì›ë³¸ ë°œì·Œë¬¸(description) ì‚¬ìš©
    summary = str(desc if desc else title)
    
    # [Nuclear Option] Cleanup and 70-char hard truncate for frontend safety
    summary = re.sub(r'[\u4e00-\u9fff]', '', summary) # Remove Hanja
    summary = re.sub(r'[\u3040-\u30ff]', '', summary) # Remove Japanese
    if len(summary) > 70:
        summary = summary[:67] + "..."
    
    impact = analysis.get("impact_level", 3)
    if isinstance(impact, dict):
        impact = 3
    
    # [2] Local Extraction for robustness
    local_main = extract_main_keyword(desc, title=title)
    local_all = extract_keywords(f"{title} {desc}", top_n=5)
    
    # [3] MERGE LOGIC (Union of AI and Local)
    final_main = ai_main if (ai_main and ai_main != "ê¸°íƒ€") else local_main
    if isinstance(final_main, dict):
        final_main = final_main.get("name", "ê¸°íƒ€") if "name" in final_main else "ê¸°íƒ€"
    final_main = str(final_main) if final_main else "ê¸°íƒ€"
    
    # [4] CLEANUP & FINAL FORMATTING (Nuclear Option)
    def clean_kw(k):
        if not k: return ""
        k = str(k).strip()
        # 1. Replace common Hanja
        k = k.replace("ç¤¾", "ì‚¬").replace("å¤–", "ì™¸").replace("å…§", "ë‚´").replace("ç¾", "ë¯¸").replace("ä¸­", "ì¤‘").replace("æ—¥", "ì¼").replace("éŸ“", "í•œ")
        
        # 2. Remove all Hanja (4E00-9FFF)
        k = re.sub(r'[\u4e00-\u9fff]', '', k)
        
        # 3. Remove all Japanese (Hiragana: 3040-309F, Katakana: 30A0-30FF)
        k = re.sub(r'[\u3040-\u30ff]', '', k)
        
        # 4. Remove strange symbols/control chars but keep basic punct
        k = re.sub(r'[^\w\s\d.,!?"\'\[\]()%&-]', '', k)
        
        corrections = {
            "íœ´zel": "íœ´ì ¤", "Hugel": "íœ´ì ¤", "íœ´ì ¤ì‚¬": "íœ´ì ¤",
            "íŒŒë§ˆë¦¬ì„œì¹˜ë°”ì´ì˜¤": "íŒŒë§ˆë¦¬ì„œì¹˜", "ë¦¬ì¥¬ë€íëŸ¬": "ë¦¬ì¥¬ë€",
            "íŒŒë§ˆë¦¬ì„œì¹˜ì‚¬": "íŒŒë§ˆë¦¬ì„œì¹˜", "ë©”ë””í†¡ìŠ¤ì‚¬": "ë©”ë””í†¡ìŠ¤"
        }
        k = corrections.get(k, k)
        return k.strip()

    # Whitelist & Language Filtering (Strict)
    pool_set = set([k.strip() for k in EXPERT_ANALYSIS_KEYWORDS])
    
    def is_valid_korean_kw(k):
        # 1. Must be in Pool
        if k not in pool_set: return False
        # 2. Must contain Hangul (Double check to avoid pure English noise)
        if not re.search(r'[ê°€-í£]', k): return False
        return True

    final_main = final_main if is_valid_korean_kw(final_main) else "ê¸°íƒ€"
    ai_included = [k for k in ai_included if is_valid_korean_kw(k)]
    local_all = [k for k in local_all if is_valid_korean_kw(k)]
    
    # 5. Summary Post-processing (Smart Clean V3)
    summary = summary.strip()
    
    # 1. Remove repetitive dots first
    summary = re.sub(r'\.{2,}', '.', summary) 
    
    # 2. Smart Cut: Remove garbage after the LAST valid sentence ending
    # Capture the last occurrence of ".ë‹¤" or ".í•¨" etc.
    # Preserve subsequent sentences if they look like proper Korean sentences, 
    # but cut if they are just numbers, symbols, or broken words.
    
    # Find the last valid sentence end index
    matches = list(re.finditer(r'([ë‹¤í•¨ìŒ]\.|[ë‹¤í•¨ìŒ])', summary))
    if matches:
        last_match = matches[-1]
        end_idx = last_match.end()
        remainder = summary[end_idx:]
        
        # If remainder is just noise (digits, %, whitespace, punctuation, short garbage)
        # We cut it. Allow remainder only if it's a long string of Hangul (another sentence).
        if not re.search(r'[ê°€-í£]{2,}', remainder):
            summary = summary[:end_idx]
            
    # 3. Final safety trim for loose garbage at ends
    summary = re.sub(r'\s+[\d.%]+\s*$', '', summary) # Remove trailing "6.95%"
    summary = summary.strip()

    final_all_kws = list(set([final_main] + ai_included + local_all))
    final_all_kws = [k for k in final_all_kws if k and k in pool_set and k not in ["ê¸°íƒ€", "-", "|", "None"]]
    
    if keyword and keyword in pool_set and keyword not in final_all_kws:
        final_all_kws.append(keyword)

    # Update Stats
    provider = analysis.get("provider", "fallback")
    STATS["total"] += 1
    STATS[provider] += 1

    # [4] FILTERING: ë§í¬ ì¤‘ë³µ + ì œëª© ìœ ì‚¬ë„ ì²´í¬ (V3.2)
    is_duplicate = False
    dup_reason = ""
    
    # Check 1: ë§í¬ ì¤‘ë³µ
    try:
        check = supabase.table("articles").select("id").eq("link", link).execute()
        if check.data:
            is_duplicate = True
            dup_reason = "Duplicate Link"
    except Exception as e:
        print(f"  âš ï¸ Supabase Link Check Error: {e}")
    
    # Check 2: ì œëª© ìœ ì‚¬ë„ (ë§í¬ ì¤‘ë³µ ì•„ë‹ ë•Œë§Œ)
    if not is_duplicate:
        dup_title = is_semantically_duplicate(title, recent_articles)
        if dup_title:
            is_duplicate = True
            dup_reason = f"Similar to: {dup_title[:20]}..."

    # [5] SAVE TO SUPABASE (ì¤‘ë³µ ì•„ë‹ ë•Œë§Œ)
    supabase_saved = False
    category = determine_category(title, desc, keyword)
    if not is_duplicate:
        try:
            prod_data = {
                "title": title, 
                "description": desc,
                "link": link,
                "published_at": pub_date, 
                "source": "Naver",
                "keyword": keyword, 
                "main_keywords": final_all_kws,
                "category": category # NEW: Category Storage
            }
            supabase.table("articles").insert(prod_data).execute()
            supabase_saved = True
            print(f"  âœ… Saved to Supabase DB (Description First)")
        except Exception as e:
            print(f"  âš ï¸ Supabase DB Error: {e}")
    else:
        print(f"  â­ï¸ Skipped ({dup_reason})")

    # [5] SAVE TO GOOGLE SHEETS (Only if Supabase saved - Perfect Sync V3.0)
    if supabase_saved:
        if worksheet:
            try:
                # [Smart Scheduling] Use System Local Time (KST)
                kst_now = datetime.datetime.now()
                now_str = kst_now.strftime("%Y-%m-%d %H:%M:%S")
                # pub_date conversion to KST...
                pd_kst_str = pub_date
                try:
                    pd_utc = datetime.datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
                    pd_kst_str = (pd_utc + datetime.timedelta(hours=9)).strftime("%Y-%m-%d %H:%M:%S")
                except: pass

                row = [now_str, keyword, category, title, link, final_main, ", ".join(final_all_kws), pd_kst_str, issue_nature, summary]
                
                try:
                    worksheet.insert_row(row, 2)
                    print(f"  ğŸ“‘ Saved to Google Sheets (Synced)")
                    # Update history for deduplication
                    recent_articles.append({'title': title, 'link': link})
                except Exception as sheet_err:
                    # [FIX] Grid ID ì—ëŸ¬ ì‹œ worksheet ì¬ì—°ê²° í›„ ì¬ì‹œë„
                    if "grid" in str(sheet_err).lower() or "insertDimension" in str(sheet_err):
                        print(f"  ğŸ”„ Google Sheet Grid Error - Reconnecting...")
                        try:
                            worksheet = get_google_sheet()
                            if worksheet:
                                worksheet.insert_row(row, 2)
                                print(f"  ğŸ“‘ Saved to Google Sheets (Reconnected)")
                                recent_articles.append({'title': title, 'link': link})
                        except Exception as retry_err:
                            print(f"  âš ï¸ Google Sheet Retry Failed: {retry_err}")
                    else:
                        print(f"  âš ï¸ Google Sheet Error (Row): {sheet_err}")
                    
            except Exception as e:
                print(f"  âš ï¸ Google Sheet Error (Total): {e}")

    # [6] Final Status Sync
    supabase.table("raw_news").update({"status": "processed"}).eq("id", raw_id).execute()
    return True

async def main():
    print(f"ğŸš€ Expert News Processor Started (Continuous Mode) at {datetime.datetime.now()}")
    
    while True:
        try:
            # 1. Fetch pending items (Batch of 20 for responsiveness)
            # ì „ì²´ ê°œìˆ˜ íŒŒì•…ì„ ìœ„í•´ count='exact' ì‚¬ìš©
            res = supabase.table("raw_news").select("*", count="exact").eq("status", "pending").limit(20).execute()
            pending_items = res.data
            total_pending = res.count
            
            if not pending_items:
                # [NEW] ai_error ìë™ ì¬ì‹œë„: ì—ëŸ¬ ê¸°ì‚¬ ì¦‰ì‹œ pendingìœ¼ë¡œ ë³€ê²½
                try:
                    retry_result = supabase.table("raw_news").update({"status": "pending"}).eq("status", "ai_error").execute()
                    if retry_result.data:
                        print(f"ğŸ”„ Auto-retry: {len(retry_result.data)} ai_error articles reset to pending")
                except Exception as retry_err:
                    pass  # ì¡°ìš©íˆ ì‹¤íŒ¨ (critical ì•„ë‹˜)
                
                # [Smart Scheduling] Use System Local Time (KST)
                kst_now = datetime.datetime.now()
                current_hour = kst_now.hour
                current_minute = kst_now.minute
                
                sleep_seconds = 60
                mode = "Default"

                # 1. 00:00 ~ 06:00 (Night: 2 hours)
                if 0 <= current_hour < 6:
                    sleep_seconds = 7200
                    mode = "Night (2h)"
                # 2. 06:00 ~ 18:30 (Day: 5 min) 
                elif 6 <= current_hour < 18 or (current_hour == 18 and current_minute < 30):
                    sleep_seconds = 300
                    mode = "Day (5m)"
                # 3. 18:30 ~ 00:00 (Evening: 10 min)
                else:
                    sleep_seconds = 600
                    mode = "Evening (10m)"

                print(f"ğŸ’¤ [{kst_now.strftime('%H:%M:%S')}] Queue empty. Sleeping {sleep_seconds}s ({mode})...")
                await asyncio.sleep(sleep_seconds)
                continue

            # 2. Sort by pub_date ASC (Process oldest first)
            pending_items.sort(key=lambda x: x.get('pub_date', ''))
            
            print(f"ğŸ” Found {len(pending_items)} items to process in this batch. (Total Pending: {total_pending})")
            
            # 3. Refresh Resources (Sheet & Context) per batch
            worksheet = get_google_sheet()
            # [V5.1.1] Use 'description' column (summary doesn't exist in Supabase articles table)
            res_recent = supabase.table("articles").select("title, description").order("published_at", desc=True).limit(300).execute()
            recent_articles = []
            for r in res_recent.data:
                recent_articles.append({"title": r['title'], "description": r['description']})
            
            # 4. Process Batch
            for item in pending_items:
                success = await process_item(item, worksheet, recent_articles)
                if success:
                    # Capture title and description for next items in this batch
                    recent_articles.append({"title": item['title'], "description": item['description']})
                await asyncio.sleep(1) # Rate limit protection

            # 5. Print Stats periodically
            if STATS["total"] > 0 and STATS["total"] % 10 == 0:
                avg_latency = sum(STATS["latencies"]) / len(STATS["latencies"]) if STATS["latencies"] else 0
                print(f"ğŸ“Š [Stats] Total: {STATS['total']} | Local: {STATS['local']} | Cloud: {STATS['cloud']} | Avg Latency: {avg_latency:.2f}s")

            # 6. Update Heartbeat (V4.6: Distinct key)
            try:
                root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                update_path = os.path.join(root_dir, "last_update.json")
                
                current_data = {}
                if os.path.exists(update_path):
                    with open(update_path, "r", encoding='utf-8') as f:
                        current_data = json.load(f)
                
                current_data["processor_heartbeat"] = datetime.datetime.now().isoformat()
                current_data["processor_status"] = "active"
                
                with open(update_path, "w", encoding='utf-8') as f:
                    json.dump(current_data, f, indent=2)
            except Exception as e:
                print(f"âš ï¸ Failed to update heartbeat: {e}")

            print("âœ… Batch complete. Pausing 5s...")
            await asyncio.sleep(5)

        except KeyboardInterrupt:
            print("\nğŸ›‘ Execution stopped by user.")
            break
        except Exception as e:
            import traceback
            print(f"âŒ Unexpected Error in Main Loop: {e}")
            try:
                import sentry_sdk
                sentry_sdk.capture_exception(e)
            except: pass
            traceback.print_exc()
            print("   -> Retrying in 30 seconds...")
            await asyncio.sleep(30)

if __name__ == "__main__":
    asyncio.run(main())
