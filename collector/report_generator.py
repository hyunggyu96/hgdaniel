# -*- coding: utf-8 -*-
"""
Report Generator V1.1:
- Queries Supabase for articles added in the last 4 hours.
- Tracks 'Collected' (raw_news) vs 'Analyzed' (articles).
- Generates a text report for email delivery.
"""

import os
import datetime
from supabase import create_client
from dotenv import load_dotenv

# Load env
env_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(env_path)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def generate_detailed_report():
    print("ğŸ“Š Generating 4-hour activities report...")
    
    # Calculate time window (4 hours ago in UTC)
    now_utc = datetime.datetime.utcnow()
    four_hrs_ago_utc = now_utc - datetime.timedelta(hours=4)
    
    # Format for display (KST)
    now_kst = now_utc + datetime.timedelta(hours=9)
    start_kst = four_hrs_ago_utc + datetime.timedelta(hours=9)
    
    time_window_str = f"{start_kst.strftime('%H:%M')} ~ {now_kst.strftime('%H:%M')}"
    iso_time_filter = four_hrs_ago_utc.isoformat()
    
    try:
        # [1] Get Collected Articles (raw_news)
        raw_res = supabase.table("raw_news").select("id", count='exact').gt("created_at", iso_time_filter).execute()
        collected_count = raw_res.count if raw_res.count is not None else 0
        
        # [2] Get AI Processed & Deployed Articles (articles)
        prod_res = supabase.table("articles").select("*").gt("created_at", iso_time_filter).execute()
        analyzed_articles = prod_res.data
        analyzed_count = len(analyzed_articles)
        
        # [3] Calculate Attempts (Heuristic: 30 min intervals = 8 attempts max in 4 hours)
        # We assume 8 attempts since it's scheduled every 30m.
        attempts = 8 

        # Format the report
        report = []
        report.append(f"ï¿½ [ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ] 4ì‹œê°„ ì •ê¸° í™œë™ ë³´ê³ ")
        report.append(f"â° ì‹œê°„ëŒ€: {time_window_str} (KST)")
        report.append(f"--------------------------------------------------")
        report.append(f"ğŸ”„ ìˆ˜ì§‘ ì‹œë„: {attempts}íšŒ (30ë¶„ ê°„ê²©)")
        report.append(f"ğŸ“¥ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {collected_count}ê±´")
        report.append(f"ğŸ§  AI ë¶„ì„ ì™„ë£Œ: {analyzed_count}ê±´")
        report.append(f"ğŸš€ ì‹œíŠ¸/í˜ì´ì§€ ë°°í¬: {analyzed_count}ê±´")
        report.append(f"--------------------------------------------------\n")
        
        if analyzed_count > 0:
            # Group by keyword for utility
            keyword_stats = {}
            high_impact = []
            
            for art in analyzed_articles:
                kw = art.get('keyword', 'ê¸°íƒ€')
                keyword_stats[kw] = keyword_stats.get(kw, 0) + 1
                if art.get('impact_level', 0) >= 4:
                    high_impact.append(art['title'])
            
            report.append("ğŸ“ˆ ì£¼ìš” í‚¤ì›Œë“œ ìš”ì•½:")
            for kw, kw_count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True):
                report.append(f" - {kw}: {kw_count}ê±´")
            
            if high_impact:
                report.append("\nğŸ”¥ ì£¼ìš” ë‰´ìŠ¤ (High Impact):")
                for title in high_impact[:5]:
                    report.append(f" - {title}")
        else:
            report.append("ğŸ’¤ í•´ë‹¹ ì‹œê°„ëŒ€ì—ëŠ” ìƒˆë¡œìš´ ì†Œì‹ì´ ì—†ìŠµë‹ˆë‹¤.")
            
        report.append(f"\nğŸ”— ì „ì²´ ëŒ€ì‹œë³´ë“œ ë³´ê¸°: https://hgdaniel.vercel.app")
        
        report_text = "\n".join(report)
        
        # Save to file for GitHub Action to read
        with open("report_body.txt", "w", encoding="utf-8") as f:
            f.write(report_text)
            
        print(f"âœ… Report generated: {collected_count} collected, {analyzed_count} analyzed.")
        return report_text

    except Exception as e:
        print(f"âŒ Error generating report: {e}")
        return None

if __name__ == "__main__":
    generate_detailed_report()
