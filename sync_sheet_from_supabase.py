"""Supabase articles â†’ ìƒˆ êµ¬ê¸€ì‹œíŠ¸ íƒ­ìœ¼ë¡œ ë™ê¸°í™”"""
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from supabase import create_client
import os
import datetime

# ì„¤ì •
SERVICE_ACCOUNT_FILE = os.path.join(os.path.dirname(__file__), 'collector', 'service_account.json')
GOOGLE_SHEET_URL = "https://docs.google.com/spreadsheets/d/1IDFVtmhu5EtxSacRqlklZo6V_x9aB0WVZIzkIx5Wkic"
SUPABASE_URL = "https://jwkdxygcpfdmavxcbcfe.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imp3a2R4eWdjcGZkbWF2eGNiY2ZlIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NjQ4NDY2NywiZXhwIjoyMDgyMDYwNjY3fQ.wpTvHzqa2yewcmBDWx-XURlMssAgOLQNr5m626R4_vo"

def sync_to_new_sheet():
    print("ğŸ”„ Supabase â†’ ìƒˆ êµ¬ê¸€ì‹œíŠ¸ ë™ê¸°í™” ì‹œì‘")
    
    # 1. Supabase ì—°ê²°
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    # 2. articles ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìµœì‹ ìˆœ)
    print("ğŸ“Š Supabase articles ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    result = supabase.table('articles').select('*').order('published_at', desc=True).execute()
    articles = result.data
    print(f"   ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    
    # 3. êµ¬ê¸€ì‹œíŠ¸ ì—°ê²°
    print("ğŸ“‘ êµ¬ê¸€ì‹œíŠ¸ ì—°ê²° ì¤‘...")
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open_by_url(GOOGLE_SHEET_URL)
    
    # 4. ìƒˆ ì‹œíŠ¸ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„±)
    new_sheet_name = "Synced_Articles"
    try:
        old_sheet = spreadsheet.worksheet(new_sheet_name)
        spreadsheet.del_worksheet(old_sheet)
        print(f"   ê¸°ì¡´ '{new_sheet_name}' ì‹œíŠ¸ ì‚­ì œ")
    except:
        pass
    
    new_worksheet = spreadsheet.add_worksheet(title=new_sheet_name, rows=len(articles)+10, cols=10)
    print(f"   ìƒˆ ì‹œíŠ¸ '{new_sheet_name}' ìƒì„± ì™„ë£Œ")
    
    # 5. í—¤ë” ì‘ì„±
    headers = ["ë¶„ì„ì‹œê°", "ê²€ìƒ‰í‚¤ì›Œë“œ", "ì¹´í…Œê³ ë¦¬", "ì œëª©", "ë§í¬", "ë©”ì¸í‚¤ì›Œë“œ", "ì „ì²´í‚¤ì›Œë“œ", "ë°œí–‰ì¼", "ì´ìŠˆì„±ê²©", "ìš”ì•½"]
    new_worksheet.append_row(headers)
    
    # 6. ë°ì´í„° ë³€í™˜ ë° ì‘ì„± (ë°°ì¹˜ë¡œ)
    print("ğŸ“ ë°ì´í„° ì‘ì„± ì¤‘...")
    rows_to_add = []
    
    for article in articles:
        # ì‹œê°„ ë³€í™˜
        pub_date = article.get('published_at', '')
        if pub_date:
            try:
                dt = datetime.datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
                pub_date_kst = (dt + datetime.timedelta(hours=9)).strftime("%Y-%m-%d %H:%M:%S")
            except:
                pub_date_kst = pub_date[:19] if len(pub_date) > 19 else pub_date
        else:
            pub_date_kst = ""
        
        # í‚¤ì›Œë“œ ì²˜ë¦¬
        main_keywords = article.get('main_keywords', [])
        if isinstance(main_keywords, list):
            keywords_str = ", ".join(main_keywords)
            main_kw = main_keywords[0] if main_keywords else ""
        else:
            keywords_str = str(main_keywords)
            main_kw = ""
        
        # í•„í„°ë§ (V5.2 ì „ìˆ˜ ê²€ì‚¬)
        CAR_BRANDS = ["ë¥´ë…¸ì½”ë¦¬ì•„", "ë¥´ë…¸ì‚¼ì„±", "í˜„ëŒ€ì°¨", "ê¸°ì•„ì°¨", "ìŒìš©ì°¨", "KGëª¨ë¹Œë¦¬í‹°", "ì‰ë³´ë ˆ", "í­ìŠ¤ë°”ê²", "ë©”ë¥´ì„¸ë°ìŠ¤", "ë²¤ì¸ ", "BMW", "ì•„ë¥´ì¹´ë‚˜", "í† ë ˆìŠ¤", "ê·¸ëœì €", "ì œë„¤ì‹œìŠ¤", "í…ŒìŠ¬ë¼"]
        NOISE = ["ì‹œìŠ¹ê¸°", "ìë™ì°¨ ë¦¬ì½œ", "íƒ€ì´ì–´ êµì²´", "ì¤‘ê³ ì°¨", "ì „ê¸°ì°¨", "ìˆ˜ì†Œì°¨", "ë„ë¡œê³µì‚¬", "ë¸”ë™ë°•ìŠ¤", "ë‹¹êµ¬(PBA)", "í”„ë¡œë†êµ¬", "í”„ë¡œë°°êµ¬"]
        BAD = ["ìºì‹œì›Œí¬", "ìºì‹œë‹¥", "ìš©ëˆí€´ì¦ˆ", "ëˆë²„ëŠ”í€´ì¦ˆ", "ì •ë‹µ", "í€´ì¦ˆ", "ì‹ ì°¨", "SUV", "A-í•„ëŸ¬", "B-í•„ëŸ¬", "C-í•„ëŸ¬", "ë””ì§€í„¸í‚¤"]
        
        full_text = f"{article.get('title', '')} {article.get('description', '')}"
        
        should_skip = False
        if any(b in full_text for b in CAR_BRANDS): should_skip = True
        elif any(n in full_text for n in NOISE): should_skip = True
        elif any(bd in article.get('title', '') for bd in BAD): should_skip = True
        
        if should_skip:
            continue

        # [V5.3] ì •í™•í•œ ì‹œê°„ í•„ë“œ ì¶”ì¶œ
        # ë¶„ì„ì‹œê° (DB ê¸°ë¡ ì‹œê°„: created_at)
        raw_created_at = article.get('created_at')
        if raw_created_at:
            try:
                # ISO í¬ë§· ë³€í™˜ (Z ë˜ëŠ” +00:00 ì²˜ë¦¬)
                c_dt = datetime.datetime.fromisoformat(str(raw_created_at).replace('Z', '+00:00'))
                created_at_kst = (c_dt + datetime.timedelta(hours=9)).strftime("%Y-%m-%d %H:%M:%S")
            except Exception as e:
                print(f"  âš ï¸ Time conversion error (created_at): {e}")
                created_at_kst = str(raw_created_at)[:19]
        else:
            # created_atì´ ì—†ìœ¼ë©´ ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ í˜„ì¬ ì‹œê° ì‚¬ìš© (ë°œí–‰ì¼ë¡œ ë®ì–´ì“°ì§€ ì•ŠìŒ)
            created_at_kst = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        row = [
            created_at_kst, # 1. ë¶„ì„ì‹œê°
            article.get('keyword', ''),
            article.get('category', 'ê¸°íƒ€'), # 3. ì¹´í…Œê³ ë¦¬ (NEW)
            article.get('title', ''),
            article.get('link', ''),
            main_kw,
            keywords_str,
            pub_date_kst,   # 8. ë°œí–‰ì¼
            article.get('issue_nature', ''),
            article.get('description', '')[:100] if article.get('description') else ""
        ]
        rows_to_add.append(row)
    
    # ë°°ì¹˜ë¡œ ì¶”ê°€ (100ê°œì”©)
    batch_size = 100
    for i in range(0, len(rows_to_add), batch_size):
        batch = rows_to_add[i:i+batch_size]
        new_worksheet.append_rows(batch)
        print(f"   {min(i+batch_size, len(rows_to_add))}/{len(rows_to_add)}ê°œ ì™„ë£Œ...")
    
    print(f"\nâœ… ì™„ë£Œ! '{new_sheet_name}' ì‹œíŠ¸ì— {len(articles)}ê°œ ê¸°ì‚¬ ë™ê¸°í™”ë¨")
    print(f"   ê¸°ì¡´ 'Sheet1'ì€ ë°±ì—…ìœ¼ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.")

if __name__ == "__main__":
    sync_to_new_sheet()
